---
title: "SMM634 Group Assignment"
subtitle: "Actuarial Science: Group 4"
author:
  - "Ardi Wira Sudarmo"
  - "Basmah Khan"
  - "Chengetanayi Dyirakumunda"
  - "Benjamin Evans"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
    self_contained: true
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    theme: flatly
    highlight: tango
    df_print: paged
---

## BE note:

Use `rmarkdown::render("MSc_AS-SMM634-Group4-Project.rmd", output_dir = "docs")`
to render...

# Initial processing

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE,
  fig.align = "center", fig.width = 7, fig.height = 5
)
```

```{r}
## dependencies / external librarys
library(dplyr)
library(ggplot2)
library(car)
library(MASS)
```

## Read Data

```{r}
rm(list = ls())
# CarDataRead.R

# source(file.path(".", "CarDataRead.R"))
df <- read.csv("car_price.csv") |>
  mutate(fsymboling = as.factor(symboling)) |>
  mutate(safety = case_match(
    symboling,
    c(-2, -1) ~ "<-1",
    0 ~ "0",
    1 ~ "1",
    2 ~ "2",
    3 ~ "3"
  )) |>
  mutate(safetyIncr = case_match(
    symboling,
    c(-2, -1) ~ "4",
    0 ~ "3",
    1 ~ "2",
    2 ~ "1",
    3 ~ "0"
  )) |>
  mutate(safetyIncr2 = case_match(
    symboling,
    0 ~ "Base",
    c(-2, -1) ~ "Safer",
    c(1, 2, 3) ~ "Riskier"
  )) |>
  mutate(cylinderNum = case_match(cylindernumber,
    c("two", "three") ~ "leq_three",
    c("eight", "twelve") ~ "geq_eight",
    .default = cylindernumber
  ))

table(df$cylinderNum)
```

### Clean data

Want to check for any typos, missing data, or NaN values.

### Car manufacturer

Extracting car manufacturer from first part of `CarName` and correcting spelling
errors.

```{r}
# extract car manufacturer from first part of CarName
df$carManufacturer <- sapply(strsplit(df$CarName, " +"), `[`, 1)
# print
table(df$carManufacturer)
```

Our assumptions are as follows:

| Typo / Mis-spelling | Correct Spelling |
| ------------------- | ---------------- |
| maxda               | mazda            |
| Nissan              | nissan           |
| porcshce            | porsche          |
| toyouta             | toyota           |
| vokswagen           | volkswagen       |
| vw                  | volkswagen       |

```{r}
df <- df %>% mutate(carManufacturer = case_match(carManufacturer,
  "maxda" ~ "mazda",
  "Nissan" ~ "nissan",
  "porcshce" ~ "porsche",
  "toyouta" ~ "toyota",
  c("vokswagen", "vw") ~ "volkswagen",
  .default = carManufacturer
))
table(df$carManufacturer)
```

### NA value check

```{r}
# check individual columns for NaN values
colSums(is.na(df))
# check total number of NaN values
cat(c("Total number of NaN values:", sum(colSums(is.na(df)))))
```

Looks good - no NaN values.

## Data examination & visualisation

### Overview

```{r}
summary(df)
str(df)
```

### Factor variables: General

```{r}
table(df$symboling)
table(df$safety)
table(df$safetyIncr)

table(df$cylindernumber)
table(df$cylinderNum)
```

```{r}
# factor_cols <- c("symboling", "safety", "safetyIncr", "cylindernumber", "cylinderNum","carbody")
# examine categorical fields
factor_cols<- c(
  "symboling",
  "safety",
  "safetyIncr",
  "safetyIncr2",
  "cylindernumber",
  "cylinderNum",
  "carbody",
  "fueltype",
  "aspiration",
  "doornumber",
  "carbody",
  "drivewheel",
  "enginelocation", # BE note: 202:3 split so don't include in model
  "enginetype",
  "cylindernumber",
  "fuelsystem",
  "carManufacturer"
)

cols_use <- unique(intersect(factor_cols, names(df)))
fc_missing <- setdiff(unique(factor_cols), names(df))
if (length(fc_missing)) {
  message(
    "Missing in df (skipped): ",
    paste(fc_missing, collapse = ", ")
  )
}
tab_list <- setNames(
  lapply(cols_use, function(v) table(df[[v]], useNA = "ifany")),
  # lapply(cols_use, function(v) prop.table(table(df[[v]], useNA = "ifany"))),
  cols_use
)
for (nm in names(tab_list)) {
  cat("\n==== ", nm, " ====\n", sep = "")
  print(tab_list[[nm]])
}
```

```{r}
df <- df %>%
  mutate(across(all_of(factor_cols), as.factor))
```

### Factor variables: check stored as factors in R

```{r}
str(df)
```

### Visualisation

Have chosen `price` as response variable. Want to see how it's distributed...
(might want to use log(price) as used in Intro to Python).

```{r}
ggplot(df, aes(x = price)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  ggtitle("Distribution of Car Prices")
```

Look at numerical variables vs price

```{r}
# price vs horsepower
ggplot(df, aes(x = horsepower, y = price)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  ggtitle("Price vs Horsepower") +
  theme_minimal()

# Price vs engine size
ggplot(df, aes(x = enginesize, y = price)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  ggtitle("Price vs Engine Size") +
  theme_minimal()

# Price vs highwaympg (could also use citympg)
ggplot(df, aes(x = highwaympg, y = price)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  ggtitle("Price vs MPG (Highway)") +
  theme_minimal()

# Price vs citympg (note: city vs highwaympg are likely to be correlated)
ggplot(df, aes(x = citympg, y = price)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  ggtitle("Price vs MPG (City)") +
  theme_minimal()

```

This is only a quick look, but clear issues with negative price assuming simple
linear regression > ~40 mpg for both city and highway mpg

```{r}
# Price vs fuel type

ggplot(df, aes(x = fueltype, y = price)) +
  geom_boxplot() +
  ggtitle("Price
Distribution by Fuel Type") +
  theme_minimal()

# Price vs car body

ggplot(df, aes(x = carbody, y = price)) +
  geom_boxplot() +
  ggtitle("Price
Distribution by Car Body") +
  theme_minimal()

# Price vs (new) manufacturer

ggplot(df, aes(x = carManufacturer, y = price)) +
  geom_boxplot() +
  ggtitle("Price Distribution by Manufacturer") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

This is not unexpected, but is quite interesting to see. Some manufacturers are
more expensive (& the range of models produced is also variable).

Based upon our personal investigation & search we think the data we have been
given here are a subset of the data from 1985 Ward's Automotive Yearbook (with a
more comprehensive data set being available from
https://archive.ics.uci.edu/dataset/10/automobile).

Note, this doesn't consider companies that own other companies (i.e., Toyota
currently owns Lexus, but both could be considered to be toyota despite both
having different price points. This consideration is something we decided was
beyond the scope of this analysis).

# Pricing Assistant Model

Our main goal is to get an accurate predicted price $$\hat{y}$$ for a new car.

BE note: A good pricing assistant chimes with goals of assessment and the
lecture content.

## Attempt 01

Excluding predictors that are highly correlated (parameter choice)

```{r}
# Y = price
# X = horsepower, enginesize, curbweight, highwaympg,
#     fueltype, aspiration, carbody, drivewheel, cylinderNum

mdl.pa1 <- lm(
  price ~ horsepower + enginesize + curbweight + highwaympg +
    fueltype + aspiration + carbody + drivewheel + cylinderNum,
  data = df
)

print(summary(mdl.pa1))
```

F-statistic: 75.54 on 16 and 188 DF, p-value: < 2.2e-16

- Test of null hypothesis that $H_0: \beta_1 = \beta_2 = \dots = 0$
- Meaning all predictors are useless
- F-value is large and p-value small so we can reject this null hypothesis

Adjusted R-squared: 0.8539

BE comments: Model accounts for ~85% of variability in the variables (Adj.
R-squared = 85.4%, F-statistic p-value < 2.2e-16).

Quite a few predictors have high p-values (not statistically significant)

Next step to use Backward Selection process to run algorithm removing the
variable with the largest p-value and re-fitting the model...

```{r}
mdl.pa2 <- stepAIC(mdl.pa1, direction = "backward", trace = TRUE)
```

### Attempt 01: Model comparison

See if variables removed as a group were significant.

Hypothesis:

- $H_0$​: The simpler model (mdl.pa2) is sufficient
- $H_1$​: The first (full) model (mdl.pa1) is significantly better.

```{r}
print(anova(mdl.pa2, mdl.pa1))
```

```{r}
par(bg = "white")
par(mfrow = c(2, 2))
plot(mdl.pa2)
par(mfrow = c(1, 1))
```

```{r}
vif(mdl.pa2)
```

Brief BE summary:

- VIF output is good - all values are below the 5 threshold, so collinearity is
  not a major issue in this model

Plots show two issues

- Heteroscedasticity in residuals vs fitted (have funnel shape as the residuals
  get larger as the predicted price increases)
- Some leverage points / large outliers

## Attempt 02

To fix heteroscedasticity let's use log(price) & look at what happens if we
include carManufacturer?

```{r}
mdl.pa3 <- lm(
  log(price) ~ horsepower + enginesize + curbweight + highwaympg +
    fueltype + aspiration + carbody + drivewheel + cylinderNum +
    carManufacturer,
  data = df
)
summary(mdl.pa3)
par(bg = "white")
par(mfrow = c(2, 2))
plot(mdl.pa3)
par(mfrow = c(1, 1))
```

```{r}
mdl.pa4 <- stepAIC(mdl.pa3, direction = "backward", trace = TRUE)
```

```{r}
summary(mdl.pa4)
```

```{r}
par(bg = "white")
par(mfrow = c(2, 2))
plot(mdl.pa4)
par(mfrow = c(1, 1))
print(vif(mdl.pa4))
```

```{r}
print(anova(mdl.pa3, mdl.pa4))
```

### Summary

- `mdl.pa4` now shows a random, horizontal band of points. The red line is flat.
  This indicates the variance of the residuals is now constant, and we can trust
  our p-values and standard errors.
- GVIF values are high, but we have lots of categories. Importantly all
  `GVIF^(1/(2*Df))` are below 5 -> no collinearity in model
- Anova test with two models fails to reject null hypothesis ($H_0$​) that all
  the variables you removed (highwaympg, fueltype, and drivewheel) have
  coefficients equal to zero and add no significant predictive value to the
  model.
  - p-value (0.7481) is large -> there is no statistical evidence that
    `highwaympg`, `fueltype`, and `drivewheel` as a group improve the model.

`mdl.pa4` (simpiler) is just as good as `mdl.pa3`, so we should use `mdl.pa4`.

## Demo: testing this model's use to answer our initial question

BE note: we know from data source that these prices are in dollars...

```{r}
# Example properties of new car
# BE note: have to use exact text for factor vals
# Also a limitation of this (not built in) is that it doesn't check for limits
# of data (i.e., if we put in a horsepower over the max value the model won't
# be valid, but this isn't accounted for here...)
# this also doesn't account for types of engine produced by companies
new_car <- data.frame(
  horsepower = 100,
  enginesize = 120,
  curbweight = 2500,
  aspiration = as.factor("std"),
  carbody = as.factor("sedan"),
  cylinderNum = as.factor("four"),
  carManufacturer = as.factor("toyota") # peugeot/volvo/porsche
)

# use predict() to estimate fit
pred_log_scale <-
  predict(mdl.pa4, newdata = new_car, interval = "prediction")

print("Prediction on log($) scale:")
print(pred_log_scale)

# convert back from log price for fit, lwr, upr
pred_dollar_scale <- exp(pred_log_scale)

print("Prediction on dollar ($) scale:")
print(pred_dollar_scale)

# All in one...
car_props <- paste(
  new_car$carManufacturer, new_car$carbody,
  "with", new_car$horsepower, "hp,",
  new_car$enginesize, "enginesize, and",
  new_car$curbweight, "lb curbweight"
)

fit_price <- pred_dollar_scale[1, "fit"]
lwr_price <- pred_dollar_scale[1, "lwr"]
upr_price <- pred_dollar_scale[1, "upr"]

report_string <- sprintf(
  "\nBased on our final model, for a %s:
  \n> The single best price estimate (yhat) is $%.0f.
  \n> We can say we are 95%% confident that the listing price for a car with
  these specific specifications would fall within prediction interval of
  [$%.0f, $%.0f].",
  car_props,
  fit_price,
  lwr_price,
  upr_price
)
cat(report_string)
```

### Demo: testing function

```{r}
get_price_prediction_report <- function(model,
                                        hp,
                                        engine_size,
                                        curb_weight,
                                        asp,
                                        body,
                                        cyl,
                                        mfr) {
  new_car <- data.frame(
    horsepower = hp,
    enginesize = engine_size,
    curbweight = curb_weight,
    aspiration = as.factor(asp),
    carbody = as.factor(body),
    cylinderNum = as.factor(cyl),
    carManufacturer = as.factor(mfr)
  )
  pred_log_scale <- predict(model, newdata = new_car, interval = "prediction")
  pred_dollar_scale <- exp(pred_log_scale)
  car_props <- paste(
    new_car$carManufacturer, new_car$carbody,
    "with", new_car$horsepower, "hp,",
    new_car$enginesize, "enginesize, and",
    new_car$curbweight, "lb curbweight"
  )
  fit_price <- pred_dollar_scale[1, "fit"]
  lwr_price <- pred_dollar_scale[1, "lwr"]
  upr_price <- pred_dollar_scale[1, "upr"]
  report_string <- sprintf(
    "\nBased on our final model, for a %s:
    > The single best price estimate (y-hat) is $%.0f
    > 95%% CI for this specific car is [$%.0f, $%.0f]",
    car_props,
    fit_price,
    lwr_price,
    upr_price
  )

  cat(report_string)
}
```

```{r}
# test 01: same toyota (to check :))
get_price_prediction_report(
  model = mdl.pa4,
  hp = 100,
  engine_size = 120,
  curb_weight = 2500,
  asp = "std",
  body = "sedan",
  cyl = "four",
  mfr = "toyota"
)

# test 02: porsche
get_price_prediction_report(
  model = mdl.pa4,
  hp = 200,
  engine_size = 180,
  curb_weight = 3000,
  asp = "turbo",
  body = "hardtop",
  cyl = "six",
  mfr = "porsche"
)

# test 03: honda
get_price_prediction_report(
  model = mdl.pa4,
  hp = 150,
  engine_size = 120,
  curb_weight = 1000,
  asp = "std",
  body = "convertible",
  cyl = "five",
  mfr = "honda"
)
```

## Limitations

Observation 76 is a high-leverage point

- coefficients are being skewed by this single observation
- /model is overfit to this point

## Suggestions for Improvements

- We could remove this point and refit the model to see if the coefficients are
  stable and the model is more general
- BE note: don't know if we want to do this in the model or leave it and discuss
  here?

### Attempt 03 (may not include in final version)

```{r}
row_to_remove <- 76

df_fixed <- df[-row_to_remove, ]

mdl.pa5 <- lm(formula(mdl.pa4), data = df_fixed)

print(summary(mdl.pa4))
print(summary(mdl.pa5))
```

```{r}
par(bg = "white")
par(mfrow = c(2, 2))
plot(mdl.pa5)
par(mfrow = c(1, 1))

cat("\n--- VIF Scores for Robust Final Model ---\n")
print(car::vif(mdl.pa5))
```

# MPG Model

## Attempt 01

Here we are asking: "How does fuel economy (citympg) affect a car's price, after
controlling for other characteristics?"

```{r}
vars <- c(
  "price", "safetyIncr", "aspiration", "carbody", "enginelocation",
  "carwidth", "curbweight", "enginetype", "cylinderNum", "enginesize",
  "stroke", "peakrpm", "compressionratio", "carManufacturer", "citympg"
)

dfm <- df[, vars]
dfm <- dfm[complete.cases(dfm), ]
dfm[] <- lapply(dfm, function(x) if (is.factor(x)) droplevels(x) else x)

# BE note: mpg1 is not included here, but have kept naming the same for
# consistency
lm.mpg2 <- lm(
  price ~ safetyIncr + aspiration + carbody +
    enginelocation + carwidth + curbweight + enginetype +
    cylinderNum + enginesize + stroke + peakrpm + compressionratio +
    carManufacturer + citympg,
  data = dfm
)
print(summary(lm.mpg2))
```

```{r}
par(bg = "white")
par(mfrow = c(2, 2))
plot(lm.mpg2)
par(mfrow = c(1, 1))

# print(car::vif(lm.mpg2))
```

BE note: vif raises an error:
`Error in vif.default(lm.mpg2): there are aliased coefficients in the model`

This (& plots) indicate:

- Perfect collinearity - model is over specified
- Heteroscedasticity from residuals vs fitted (general & 67, 17 and 75)
- Leverage points (50)

Model gives p-value of 0.107634, which suggests effect of `citympg` is not
significant, but errors stated above indicate/suggest model is not valid

### Attempting to address these issues

```{r}
# use log(price)
lm.mpg2log <- lm(
  log(price) ~ safetyIncr + aspiration + carbody +
    enginelocation + carwidth + curbweight + enginetype +
    cylinderNum + enginesize + stroke + peakrpm + compressionratio +
    carManufacturer + citympg,
  data = dfm
)
summary(lm.mpg2log)
```

```{r}
lm.mpg2logaic <- stepAIC(lm.mpg2log, direction = "backward", trace = FALSE)
print(summary(lm.mpg2logaic))
```

```{r}
par(bg = "white")
par(mfrow = c(2, 2))
plot(lm.mpg2logaic)
par(mfrow = c(1, 1))
```

```{r}
print(car::vif(lm.mpg2logaic))
print(anova(lm.mpg2logaic, lm.mpg2log))
```

BE notes:

- Heteroscedasticity is addressed (residuals vs fitted)
- Adjusted r-squared is high (0.9448), model explains 94% of variance in the
  logarithm of the price
- All VIF values are below threshold of 5 (although curbweight is slightly high)
- Anova shows simplier model is sufficient -> this is the one we should use

Linking back to original question: "How does fuel economy (citympg) affect a
car's price, after controlling for other characteristics?"

Linking back to original question: "How does fuel economy (citympg) affect a
car's price, after controlling for other characteristics?"

**Coefficients**

| Term    |   Estimate | Std. Error | t value | Pr(> t ) |
| ------- | ---------: | ---------: | ------: | -------: |
| citympg | -4.162e-03 |  3.027e-03 |  -1.375 | 0.171028 |

Answer:

- Estimate: $-4.163\times10^{-3}$
- p-value: $0.171$

So `citympg` does not have a statistically significant effect on log(price)
after controlling for other factors (like curbweight, enginesize, carwidth, and
carManufacturer)

Answer:

- Estimate: $-4.163\times10^{-3}$
- p-value: $0.171$

So `citympg` does not have a statistically significant effect on log(price)
after controlling for other factors (like curbweight, enginesize, carwidth, and
carManufacturer)

## Attempt 02

Note here Ardi's great model looks to address the question: 'What car features
predict a car's fuel efficiency value?'

BE note: uses `log(highwaympg/price)` which represents MPG per dollar (or fuel
efficiency value)

```{r}
lm.mpg3 <- lm(
  formula = log(highwaympg / price) ~ carbody + enginetype +
    carwidth + curbweight + peakrpm + horsepower + carManufacturer,
  data = df
)

summary(lm.mpg3)

par(bg = "white")
par(mfrow = c(2, 2))
plot(lm.mpg3)

print(vif(lm.mpg3))
```

### Problem

BE note: Is the fact that we have the message “not plotting observations with
leverage one: 19, 76, 126, 130” a problem?

```{r}
lm.mpg3aic <- stepAIC(lm.mpg3, direction = "backward", trace = TRUE)
print(summary(lm.mpg3aic))
par(bg = "white")
par(mfrow = c(2, 2))
plot(lm.mpg3aic)
vif(lm.mpg3aic)
```

Summary

- Model explains ~95% of variance in fuel efficiency value
- Heteroscedasticity: residuals vs fitted looks good -> no heteroscedasticity
- Collinearity values are all below threshold of 5

Not sure if we want to mention it's flawed due to leverage points and then tell
story.

Story is quite clear though:

- `curbweight` and `horsepower` are the strongest predictors with significant
  negative coefficients -> makes sense:
  - heavier (up) -> mpg per dollar (down)
  - more powerful (horsepower up) -> mpg per dollar (down)

Rotor engine has strong negative coefficient -> indicating it has a poor fuel
efficiency value -> consistent with prior knowledge

## Limitations

TBI

## Suggestions for Improvements

TBI
